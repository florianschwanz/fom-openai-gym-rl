{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import FileWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Make library available in path\n",
    "!rm -rf 'fom-openai-gym-rl'\n",
    "!git clone https://github.com/fom-big-data/fom-openai-gym-rl\n",
    "lib_path = os.path.join(os.getcwd(), 'fom-openai-gym-rl', 'notebooks', '00-basemodel', 'atari-dqn', 'lib')\n",
    "if not (lib_path in sys.path):\n",
    "    sys.path.insert(0, lib_path)\n",
    "\n",
    "# Import library classes\n",
    "from replay_memory import ReplayMemory\n",
    "from deep_q_network import DeepQNetwork\n",
    "from action_selector import ActionSelector\n",
    "from input_extractor import InputExtractor\n",
    "from model_optimizer import ModelOptimizer\n",
    "from model_storage import ModelStorage\n",
    "from environment_enum import Environment\n",
    "from pong_reward_shaper import PongRewardShaper\n",
    "from reward_shape_enum import RewardShape\n",
    "from performance_logger import PerformanceLogger\n",
    "from environment_builder import EnvironmentBuilder\n",
    "from environment_builder import EnvironmentWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0 Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ENVIRONMENT_NAME = Environment.PONG_NO_FRAMESKIP_v4\n",
    "ENVIRONMENT_WRAPPERS = [\n",
    "    EnvironmentWrapper.NOOP_RESET_ENV,\n",
    "    EnvironmentWrapper.MAX_AND_SKIP_ENV,\n",
    "    # EnvironmentWrapper.FRAME_STACK,\n",
    "]\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 5\n",
    "REPLAY_MEMORY_SIZE = 10_000\n",
    "NUM_FRAMES = 1_000_000\n",
    "REWARD_SHAPINGS = [\n",
    "    RewardShape.PONG_PLAYER_RACKET_HITS_BALL,\n",
    "    RewardShape.PONG_PLAYER_RACKET_CLOSE_TO_BALL_LINEAR,\n",
    "    RewardShape.PONG_OPPONENT_RACKET_HITS_BALL,\n",
    "    RewardShape.PONG_OPPONENT_RACKET_CLOSE_TO_BALL_LINEAR,\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0.1 Configure device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0.2 Set up matplotlib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Enable interactive mode of matplotlib\n",
    "plt.ion()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0.3 Set up TensorBoard"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0.4 Set up environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = EnvironmentBuilder.make_environment_with_wrappers(ENVIRONMENT_NAME.value, ENVIRONMENT_WRAPPERS)\n",
    "# Reset environment\n",
    "env.reset()\n",
    "# Plot initial screen\n",
    "InputExtractor.plot_screen(InputExtractor.get_sharp_screen(env=env, device=device), 'Example extracted screen')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Set up nets\n",
    "\n",
    "# 1.1 Define nets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = InputExtractor.get_screen(env=env, device=device)\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Initialize policy net and target net\n",
    "policy_net = DeepQNetwork(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DeepQNetwork(screen_height, screen_width, n_actions).to(device)\n",
    "\n",
    "# Since both nets are initialized randomly we need to copy the state of one into the other to make sure they are equal\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.2 Define optimizer and replay memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "# Initialize replay memory\n",
    "memory = ReplayMemory(REPLAY_MEMORY_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize total variables\n",
    "total_frames = 0\n",
    "total_episodes = 0\n",
    "total_original_rewards = []\n",
    "total_shaped_rewards = []\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Initialize episode variables\n",
    "episode_frames = 0\n",
    "episode_original_reward = 0\n",
    "episode_shaped_reward = 0\n",
    "episode_start_time = time.time()\n",
    "\n",
    "# Initialize the environment and state\n",
    "env.reset()\n",
    "last_screen = InputExtractor.get_screen(env=env, device=device)\n",
    "current_screen = InputExtractor.get_screen(env=env, device=device)\n",
    "state = current_screen - last_screen"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.1 Display TensorBoard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize writer\n",
    "tensorboard_summary_writer = SummaryWriter()\n",
    "tensorboard_file_writer = FileWriter(\"images\")\n",
    "%tensorboard --logdir=runs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.2 Training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over frames\n",
    "progress_bar = tqdm(range(NUM_FRAMES), unit='frames')\n",
    "for total_frames in progress_bar:\n",
    "\n",
    "    # Select and perform an action\n",
    "    action = ActionSelector.select_action(state=state,\n",
    "                                          n_actions=n_actions,\n",
    "                                          policy_net=policy_net,\n",
    "                                          epsilon_end=EPS_END,\n",
    "                                          epsilon_start=EPS_START,\n",
    "                                          epsilon_decay=EPS_DECAY,\n",
    "                                          device=device)\n",
    "\n",
    "    # Do step\n",
    "    observation, reward, done, info = env.step(action.item())\n",
    "    \n",
    "    # Unwrap observations if frame stack is in use\n",
    "    if EnvironmentWrapper.FRAME_STACK in ENVIRONMENT_WRAPPERS:\n",
    "        print(\"Not yet supported\")\n",
    "        exit\n",
    "\n",
    "    # Shape reward\n",
    "    original_reward = reward\n",
    "    shaped_reward = reward\n",
    "\n",
    "    if ENVIRONMENT_NAME == Environment.PONG_v0 \\\n",
    "            or ENVIRONMENT_NAME == Environment.PONG_v4 \\\n",
    "            or ENVIRONMENT_NAME == Environment.PONG_DETERMINISTIC_v0 \\\n",
    "            or ENVIRONMENT_NAME == Environment.PONG_DETERMINISTIC_v4 \\\n",
    "            or ENVIRONMENT_NAME == Environment.PONG_NO_FRAMESKIP_v0 \\\n",
    "            or ENVIRONMENT_NAME == Environment.PONG_NO_FRAMESKIP_v4:\n",
    "        # # Plot intermediate screen after scoring\n",
    "        # InputExtractor.plot_screen(InputExtractor.get_sharp_screen(env=env, device=device), \"Frame \" + str(\n",
    "        #     total_frames) + \" / reward \" + str(round(reward, 4)) + \" / GOOOAAAAL!!!\")\n",
    "        \n",
    "        reward_shaper = PongRewardShaper(observation, reward, done, info)\n",
    "\n",
    "        if RewardShape.PONG_PLAYER_RACKET_HITS_BALL in REWARD_SHAPINGS:\n",
    "            additional_reward = reward_shaper.reward_player_racket_hits_ball()\n",
    "            # if additional_reward != 0:\n",
    "            #     # Plot screen after additional reward has been given\n",
    "            #     InputExtractor.plot_screen(InputExtractor.get_sharp_screen(env=env, device=device),\n",
    "            #                                str(total_frames) + \" / Player hits ball\")\n",
    "            shaped_reward += additional_reward\n",
    "        if RewardShape.PONG_PLAYER_RACKET_COVERS_BALL in REWARD_SHAPINGS:\n",
    "            shaped_reward += reward_shaper.reward_player_racket_covers_ball()\n",
    "        if RewardShape.PONG_PLAYER_RACKET_CLOSE_TO_BALL_LINEAR in REWARD_SHAPINGS:\n",
    "            shaped_reward += reward_shaper.reward_player_racket_close_to_ball_linear()\n",
    "        if RewardShape.PONG_PLAYER_RACKET_CLOSE_TO_BALL_QUADRATIC in REWARD_SHAPINGS:\n",
    "            shaped_reward += reward_shaper.reward_player_racket_close_to_ball_quadractic()\n",
    "\n",
    "        if RewardShape.PONG_OPPONENT_RACKET_HITS_BALL in REWARD_SHAPINGS:\n",
    "            additional_reward = reward_shaper.reward_opponent_racket_hits_ball()\n",
    "            # if additional_reward != 0:\n",
    "            #     # Plot screen after additional reward has been given\n",
    "            #     InputExtractor.plot_screen(InputExtractor.get_sharp_screen(env=env, device=device),\n",
    "            #                                str(total_frames) + \" / Opponent hits ball\")\n",
    "            shaped_reward += additional_reward\n",
    "        if RewardShape.PONG_OPPONENT_RACKET_COVERS_BALL in REWARD_SHAPINGS:\n",
    "            shaped_reward += reward_shaper.reward_opponent_racket_covers_ball()\n",
    "        if RewardShape.PONG_OPPONENT_RACKET_CLOSE_TO_BALL_LINEAR in REWARD_SHAPINGS:\n",
    "            shaped_reward += reward_shaper.reward_opponent_racket_close_to_ball_linear()\n",
    "        if RewardShape.PONG_OPPONENT_RACKET_CLOSE_TO_BALL_QUADRATIC in REWARD_SHAPINGS:\n",
    "            shaped_reward += reward_shaper.reward_opponent_racket_close_to_ball_quadractic()\n",
    "\n",
    "\n",
    "    # # Plot intermediate screen\n",
    "    # if total_frames % 50 == 0:\n",
    "    #     InputExtractor.plot_screen(InputExtractor.get_sharp_screen(env=env, device=device), \"Frame \" + str(\n",
    "    #         total_frames) + \" / shaped reward \" + str(round(shaped_reward, 4)))\n",
    "        \n",
    "    # Use shaped reward for further processing\n",
    "    reward = shaped_reward\n",
    "\n",
    "    # Add reward to episode reward\n",
    "    episode_original_reward += original_reward\n",
    "    episode_shaped_reward += shaped_reward\n",
    "\n",
    "    # Transform reward into a tensor\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "\n",
    "    # Observe new state\n",
    "    last_screen = current_screen\n",
    "    current_screen = InputExtractor.get_screen(env=env, device=device)\n",
    "\n",
    "    if not done:\n",
    "        next_state = current_screen - last_screen\n",
    "    else:\n",
    "        next_state = None\n",
    "\n",
    "    # Store the transition in memory\n",
    "    memory.push(state, action, next_state, reward)\n",
    "\n",
    "    # Move to the next state\n",
    "    state = next_state\n",
    "\n",
    "    # Perform one step of the optimization (on the target network)\n",
    "    loss = ModelOptimizer.optimize_model(policy_net=policy_net,\n",
    "                                         target_net=target_net,\n",
    "                                         optimizer=optimizer,\n",
    "                                         memory=memory,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         gamma=GAMMA,\n",
    "                                         device=device)\n",
    "\n",
    "    if done:    \n",
    "        # Track episode time\n",
    "        episode_end_time = time.time()\n",
    "        episode_duration = episode_end_time - episode_start_time\n",
    "        total_duration = episode_end_time - total_start_time\n",
    "\n",
    "        # Add rewards to total reward\n",
    "        total_original_rewards.append(episode_original_reward)\n",
    "        total_shaped_rewards.append(episode_shaped_reward)\n",
    "\n",
    "        if loss is not None:\n",
    "            PerformanceLogger.log_episode_short(total_episodes=total_episodes + 1,\n",
    "                                                total_frames=total_frames,\n",
    "                                                total_duration=total_duration,\n",
    "                                                total_original_rewards=total_original_rewards,\n",
    "                                                total_shaped_rewards=total_shaped_rewards,\n",
    "                                                episode_frames=episode_frames + 1,\n",
    "                                                episode_original_reward=episode_original_reward,\n",
    "                                                episode_shaped_reward=episode_shaped_reward,\n",
    "                                                episode_loss=loss.item(),\n",
    "                                                episode_duration=episode_duration)\n",
    "\n",
    "        # Update the target network, copying all weights and biases from policy net into target net\n",
    "        if total_episodes % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "            # Save model\n",
    "            ModelStorage.saveModel(total_frames=total_frames,\n",
    "                           net=target_net,\n",
    "                           optimizer=optimizer,\n",
    "                           loss=loss,\n",
    "                           environment_name=ENVIRONMENT_NAME,\n",
    "                           environment_wrappers=ENVIRONMENT_WRAPPERS,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           gamma=GAMMA,\n",
    "                           eps_start=EPS_START,\n",
    "                           eps_end=EPS_END,\n",
    "                           eps_decay=EPS_DECAY,\n",
    "                           target_update=TARGET_UPDATE,\n",
    "                           replay_memory_size=REPLAY_MEMORY_SIZE,\n",
    "                           num_frames=NUM_FRAMES,\n",
    "                           reward_shapings=REWARD_SHAPINGS\n",
    "                           )\n",
    "\n",
    "        # Reset episode variables\n",
    "        episode_frames = 0\n",
    "        episode_original_reward = 0\n",
    "        episode_shaped_reward = 0\n",
    "        episode_start_time = time.time()\n",
    "        \n",
    "        # Reset the environment and state\n",
    "        env.reset()\n",
    "        last_screen = InputExtractor.get_screen(env=env, device=device)\n",
    "        current_screen = InputExtractor.get_screen(env=env, device=device)\n",
    "        state = current_screen - last_screen\n",
    "\n",
    "        # Increment counter\n",
    "        total_episodes += 1\n",
    "\n",
    "    # Increment counter\n",
    "    episode_frames += 1\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}