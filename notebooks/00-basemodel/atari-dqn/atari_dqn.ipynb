{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import FileWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Make library available in path\n",
    "!rm -rf 'fom-openai-gym-rl'\n",
    "!git clone https://github.com/fom-big-data/fom-openai-gym-rl\n",
    "lib_path = os.path.join(os.getcwd(), 'fom-openai-gym-rl', 'notebooks', '00-basemodel', 'atari-dqn', 'lib')\n",
    "if not (lib_path in sys.path):\n",
    "    sys.path.insert(0, lib_path)\n",
    "common_lib_path = os.path.join(os.getcwd(), 'fom-openai-gym-rl', 'notebooks', '00-basemodel', 'common', 'lib')\n",
    "if not (common_lib_path in sys.path):\n",
    "    sys.path.insert(0, common_lib_path)\n",
    "common_reward_shaper_path = os.path.join(os.getcwd(), 'fom-openai-gym-rl', 'notebooks', '00-basemodel', 'common', 'reward_shaper')\n",
    "if not (common_reward_shaper_path in sys.path):\n",
    "    sys.path.insert(0, common_reward_shaper_path)\n",
    "    \n",
    "# Make directory for models    \n",
    "!mkdir -p ./model\n",
    "\n",
    "# Import library classes\n",
    "from action_selector import ActionSelector\n",
    "from breakout_reward_shaper import BreakoutRewardShaper\n",
    "from deep_q_network import DeepQNetwork\n",
    "from environment_builder import EnvironmentBuilder\n",
    "from environment_builder import EnvironmentWrapper\n",
    "from environment_enum import Environment\n",
    "from freeway_reward_shaper import FreewayRewardShaper\n",
    "from input_extractor import InputExtractor\n",
    "from model_optimizer import ModelOptimizer\n",
    "from model_storage import ModelStorage\n",
    "from performance_logger import PerformanceLogger\n",
    "from performance_plotter import PerformancePlotter\n",
    "from pong_reward_shaper import PongRewardShaper\n",
    "from replay_memory import ReplayMemory\n",
    "from screen_plotter import ScreenPlotter\n",
    "from spaceinvaders_reward_shaper import SpaceInvadersRewardShaper"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0 Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Path to model to be loaded\n",
    "RUN_TO_LOAD = None\n",
    "OUTPUT_DIRECTORY = \"./model/\"\n",
    "\n",
    "if RUN_TO_LOAD != None:\n",
    "    # Get latest file from run\n",
    "    list_of_files = glob.glob(OUTPUT_DIRECTORY + RUN_TO_LOAD + \"/*.model\")\n",
    "    MODEL_TO_LOAD = max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "    RUN_DIRECTORY = RUN_TO_LOAD\n",
    "\n",
    "    FINISHED_FRAMES, \\\n",
    "    FINISHED_EPISODES, \\\n",
    "    MODEL_STATE_DICT, \\\n",
    "    OPTIMIZER_STATE_DICT, \\\n",
    "    REPLAY_MEMORY, \\\n",
    "    LOSS, \\\n",
    " \\\n",
    "    ENVIRONMENT, \\\n",
    "    ENVIRONMENT_WRAPPERS, \\\n",
    "    BATCH_SIZE, \\\n",
    "    GAMMA, \\\n",
    "    EPS_START, \\\n",
    "    EPS_END, \\\n",
    "    EPS_DECAY, \\\n",
    "    NUM_ATOMS, \\\n",
    "    VMIN, \\\n",
    "    VMAX, \\\n",
    "    TARGET_UPDATE_RATE, \\\n",
    "    MODEL_SAVE_RATE, \\\n",
    "    REPLAY_MEMORY_SIZE, \\\n",
    "    NUM_FRAMES, \\\n",
    "    REWARD_PONG_PLAYER_RACKET_HITS_BALL, \\\n",
    "    REWARD_PONG_PLAYER_RACKET_COVERS_BALL, \\\n",
    "    REWARD_PONG_PLAYER_RACKET_CLOSE_TO_BALL_LINEAR, \\\n",
    "    REWARD_PONG_PLAYER_RACKET_CLOSE_TO_BALL_QUADRATIC, \\\n",
    "    REWARD_PONG_OPPONENT_RACKET_HITS_BALL, \\\n",
    "    REWARD_PONG_OPPONENT_RACKET_COVERS_BALL, \\\n",
    "    REWARD_PONG_OPPONENT_RACKET_CLOSE_TO_BALL_LINEAR, \\\n",
    "    REWARD_PONG_OPPONENT_RACKET_CLOSE_TO_BALL_QUADRATIC, \\\n",
    "    REWARD_BREAKOUT_PLAYER_RACKET_HITS_BALL, \\\n",
    "    REWARD_BREAKOUT_PLAYER_RACKET_COVERS_BALL, \\\n",
    "    REWARD_BREAKOUT_PLAYER_RACKET_CLOSE_TO_BALL_LINEAR, \\\n",
    "    REWARD_BREAKOUT_PLAYER_RACKET_CLOSE_TO_BALL_QUADRATIC, \\\n",
    "    REWARD_SPACEINVADERS_PLAYER_AVOIDS_LINE_OF_FIRE, \\\n",
    "    REWARD_FREEWAY_CHICKEN_VERTICAL_POSITION \\\n",
    "        = ModelStorage.loadModel(MODEL_TO_LOAD)\n",
    "else:\n",
    "    RUN_DIRECTORY = datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "    # Only use defined parameters if there is no previous output being loaded\n",
    "    FINISHED_FRAMES = 0\n",
    "    FINISHED_EPISODES = 0\n",
    "\n",
    "    # Define setup\n",
    "    ENVIRONMENT_ID = os.getenv('ENVIRONMENT_ID', Environment.BREAKOUT_NO_FRAMESKIP_V0.value)\n",
    "    ENVIRONMENT = Environment(ENVIRONMENT_ID)\n",
    "    ENVIRONMENT_WRAPPERS = [\n",
    "        EnvironmentWrapper.KEEP_ORIGINAL_OBSERVATION,\n",
    "        EnvironmentWrapper.NOOP_RESET,\n",
    "        EnvironmentWrapper.MAX_AND_SKIP,\n",
    "        EnvironmentWrapper.EPISODIC_LIFE,\n",
    "        EnvironmentWrapper.FIRE_RESET,\n",
    "        EnvironmentWrapper.WARP_FRAME,\n",
    "        EnvironmentWrapper.IMAGE_TO_PYTORCH,\n",
    "    ]\n",
    "    BATCH_SIZE = 32\n",
    "    GAMMA = 0.99\n",
    "    EPS_START = 1.0\n",
    "    EPS_END = 0.01\n",
    "    EPS_DECAY = 10_000\n",
    "    NUM_ATOMS = 51\n",
    "    VMIN = -10\n",
    "    VMAX = 10\n",
    "    TARGET_UPDATE_RATE = 10_000\n",
    "    MODEL_SAVE_RATE = 100\n",
    "    REPLAY_MEMORY_SIZE = 100_000\n",
    "    NUM_FRAMES = 1_000_000\n",
    "\n",
    "    REWARD_PONG_PLAYER_RACKET_HITS_BALL = 0.0\n",
    "    REWARD_PONG_PLAYER_RACKET_COVERS_BALL = 0.0\n",
    "    REWARD_PONG_PLAYER_RACKET_CLOSE_TO_BALL_LINEAR = 0.0\n",
    "    REWARD_PONG_PLAYER_RACKET_CLOSE_TO_BALL_QUADRATIC = 0.0\n",
    "    REWARD_PONG_OPPONENT_RACKET_HITS_BALL = 0.0\n",
    "    REWARD_PONG_OPPONENT_RACKET_COVERS_BALL = 0.0\n",
    "    REWARD_PONG_OPPONENT_RACKET_CLOSE_TO_BALL_LINEAR = 0.0\n",
    "    REWARD_PONG_OPPONENT_RACKET_CLOSE_TO_BALL_QUADRATIC = 0.0\n",
    "    REWARD_BREAKOUT_PLAYER_RACKET_HITS_BALL = 0.0\n",
    "    REWARD_BREAKOUT_PLAYER_RACKET_COVERS_BALL = 0.0\n",
    "    REWARD_BREAKOUT_PLAYER_RACKET_CLOSE_TO_BALL_LINEAR = 0.0\n",
    "    REWARD_BREAKOUT_PLAYER_RACKET_CLOSE_TO_BALL_QUADRATIC = 0.0\n",
    "    REWARD_SPACEINVADERS_PLAYER_AVOIDS_LINE_OF_FIRE = 0.0\n",
    "    REWARD_FREEWAY_CHICKEN_VERTICAL_POSITION = 0.0\n",
    "\n",
    "    # Log parameters\n",
    "    PerformanceLogger.log_parameters(directory=OUTPUT_DIRECTORY + RUN_DIRECTORY,\n",
    "                                     batch_size=BATCH_SIZE,\n",
    "                                     gamma=GAMMA,\n",
    "                                     eps_start=EPS_START,\n",
    "                                     eps_end=EPS_END,\n",
    "                                     eps_decay=EPS_END,\n",
    "                                     num_atoms=NUM_ATOMS,\n",
    "                                     vmin=VMIN,\n",
    "                                     vmax=VMAX,\n",
    "                                     target_update_rate=TARGET_UPDATE_RATE,\n",
    "                                     model_save_rate=MODEL_SAVE_RATE,\n",
    "                                     replay_memory_size=REPLAY_MEMORY_SIZE,\n",
    "                                     num_frames=NUM_FRAMES,\n",
    "                                     reward_pong_player_racket_hits_ball=REWARD_PONG_PLAYER_RACKET_HITS_BALL,\n",
    "                                     reward_pong_player_racket_covers_ball=REWARD_PONG_PLAYER_RACKET_COVERS_BALL,\n",
    "                                     reward_pong_player_racket_close_to_ball_linear=REWARD_PONG_PLAYER_RACKET_CLOSE_TO_BALL_LINEAR,\n",
    "                                     reward_pong_player_racket_close_to_ball_quadratic=REWARD_PONG_PLAYER_RACKET_CLOSE_TO_BALL_QUADRATIC,\n",
    "                                     reward_pong_opponent_racket_hits_ball=REWARD_PONG_OPPONENT_RACKET_HITS_BALL,\n",
    "                                     reward_pong_opponent_racket_covers_ball=REWARD_PONG_OPPONENT_RACKET_COVERS_BALL,\n",
    "                                     reward_pong_opponent_racket_close_to_ball_linear=REWARD_PONG_OPPONENT_RACKET_CLOSE_TO_BALL_LINEAR,\n",
    "                                     reward_pong_opponent_racket_close_to_ball_quadratic=REWARD_PONG_OPPONENT_RACKET_CLOSE_TO_BALL_QUADRATIC,\n",
    "                                     reward_breakout_player_racket_hits_ball=REWARD_BREAKOUT_PLAYER_RACKET_HITS_BALL,\n",
    "                                     reward_breakout_player_racket_covers_ball=REWARD_BREAKOUT_PLAYER_RACKET_COVERS_BALL,\n",
    "                                     reward_breakout_player_racket_close_to_ball_linear=REWARD_BREAKOUT_PLAYER_RACKET_CLOSE_TO_BALL_LINEAR,\n",
    "                                     reward_breakout_player_racket_close_to_ball_quadratic=REWARD_BREAKOUT_PLAYER_RACKET_CLOSE_TO_BALL_QUADRATIC,\n",
    "                                     reward_spaceinvaders_player_avoids_line_of_fire=REWARD_SPACEINVADERS_PLAYER_AVOIDS_LINE_OF_FIRE,\n",
    "                                     reward_freeway_chicken_vertical_position=REWARD_FREEWAY_CHICKEN_VERTICAL_POSITION\n",
    "                                     )\n",
    "# Assemble reward shapings\n",
    "REWARD_SHAPINGS = [\n",
    "    {\"method\": PongRewardShaper().reward_player_racket_hits_ball,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_PONG_PLAYER_RACKET_HITS_BALL}},\n",
    "    {\"method\": PongRewardShaper().reward_player_racket_covers_ball,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_PONG_PLAYER_RACKET_COVERS_BALL}},\n",
    "    {\"method\": PongRewardShaper().reward_player_racket_close_to_ball_linear,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_PONG_PLAYER_RACKET_CLOSE_TO_BALL_LINEAR}},\n",
    "    {\"method\": PongRewardShaper().reward_player_racket_close_to_ball_quadratic,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_PONG_PLAYER_RACKET_CLOSE_TO_BALL_QUADRATIC}},\n",
    "    {\"method\": PongRewardShaper().reward_opponent_racket_hits_ball,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_PONG_OPPONENT_RACKET_HITS_BALL}},\n",
    "    {\"method\": PongRewardShaper().reward_opponent_racket_covers_ball,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_PONG_OPPONENT_RACKET_COVERS_BALL}},\n",
    "    {\"method\": PongRewardShaper().reward_opponent_racket_close_to_ball_linear,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_PONG_OPPONENT_RACKET_CLOSE_TO_BALL_LINEAR}},\n",
    "    {\"method\": PongRewardShaper().reward_opponent_racket_close_to_ball_quadratic,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_PONG_OPPONENT_RACKET_CLOSE_TO_BALL_QUADRATIC}},\n",
    "    {\"method\": BreakoutRewardShaper().reward_player_racket_hits_ball,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_BREAKOUT_PLAYER_RACKET_HITS_BALL}},\n",
    "    {\"method\": BreakoutRewardShaper().reward_player_racket_covers_ball,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_BREAKOUT_PLAYER_RACKET_COVERS_BALL}},\n",
    "    {\"method\": BreakoutRewardShaper().reward_player_racket_close_to_ball_linear,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_BREAKOUT_PLAYER_RACKET_CLOSE_TO_BALL_LINEAR}},\n",
    "    {\"method\": BreakoutRewardShaper().reward_player_racket_close_to_ball_quadratic,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_BREAKOUT_PLAYER_RACKET_CLOSE_TO_BALL_QUADRATIC}},\n",
    "    {\"method\": SpaceInvadersRewardShaper().reward_player_avoids_line_of_fire,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_SPACEINVADERS_PLAYER_AVOIDS_LINE_OF_FIRE}},\n",
    "    {\"method\": FreewayRewardShaper().reward_chicken_vertical_position,\n",
    "     \"arguments\": {\"additional_reward\": REWARD_FREEWAY_CHICKEN_VERTICAL_POSITION}},\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0.1 Configure device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0.2 Set up matplotlib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Enable interactive mode of matplotlib\n",
    "plt.ion()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0.3 Set up TensorBoard"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0.4 Set up environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = EnvironmentBuilder.make_environment_with_wrappers(ENVIRONMENT.value, ENVIRONMENT_WRAPPERS)\n",
    "# Reset environment\n",
    "env.reset()\n",
    "# Plot initial screen\n",
    "# InputExtractor.plot_screen(InputExtractor.get_sharp_screen(env=env, device=device), 'Example extracted screen')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Set up nets\n",
    "\n",
    "# 1.1 Define nets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = InputExtractor.get_screen(env=env, device=device)\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Only use defined parameters if there is no previous model being loaded\n",
    "if RUN_TO_LOAD != None:\n",
    "    # Initialize and loade policy net and target net\n",
    "    policy_net = DeepQNetwork(screen_height, screen_width, n_actions).to(device)\n",
    "    policy_net.load_state_dict(MODEL_STATE_DICT)\n",
    "\n",
    "    target_net = DeepQNetwork(screen_height, screen_width, n_actions).to(device)\n",
    "    target_net.load_state_dict(MODEL_STATE_DICT)\n",
    "else:\n",
    "    # Initialize policy net and target net\n",
    "    policy_net = DeepQNetwork(screen_height, screen_width, n_actions).to(device)\n",
    "\n",
    "    target_net = DeepQNetwork(screen_height, screen_width, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.2 Define optimizer and replay memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Only use defined parameters if there is no previous model being loaded\n",
    "if RUN_TO_LOAD != None:\n",
    "    # Initialize and load optimizer\n",
    "    optimizer = optim.RMSprop(policy_net.parameters())\n",
    "    optimizer.load_state_dict(OPTIMIZER_STATE_DICT)\n",
    "\n",
    "    # Load memory\n",
    "    memory = REPLAY_MEMORY\n",
    "else:\n",
    "    # Initialize optimizer\n",
    "    optimizer = optim.RMSprop(policy_net.parameters())\n",
    "    # Initialize replay memory\n",
    "    memory = ReplayMemory(REPLAY_MEMORY_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize total variables\n",
    "total_frames = 0\n",
    "total_episodes = FINISHED_EPISODES\n",
    "total_original_rewards = []\n",
    "total_shaped_rewards = []\n",
    "total_losses = []\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Initialize episode variables\n",
    "episode_frames = 0\n",
    "episode_original_reward = 0\n",
    "episode_shaped_reward = 0\n",
    "episode_start_time = time.time()\n",
    "\n",
    "# Initialize the environment and state\n",
    "env.reset()\n",
    "last_screen = InputExtractor.get_screen(env=env, device=device)\n",
    "current_screen = InputExtractor.get_screen(env=env, device=device)\n",
    "state = current_screen - last_screen"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.1 Display TensorBoard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize writer\n",
    "tensorboard_summary_writer = SummaryWriter()\n",
    "tensorboard_file_writer = FileWriter(\"images\")\n",
    "%tensorboard --logdir=runs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.2 Training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over frames\n",
    "progress_bar = tqdm(iterable=range(NUM_FRAMES), unit='frames', initial=FINISHED_FRAMES)\n",
    "for total_frames in progress_bar:\n",
    "    total_frames += FINISHED_FRAMES\n",
    "\n",
    "    # Select and perform an action\n",
    "    action = ActionSelector.select_action(state=state,\n",
    "                                          n_actions=n_actions,\n",
    "                                          total_frames=total_frames,\n",
    "                                          policy_net=policy_net,\n",
    "                                          epsilon_end=EPS_END,\n",
    "                                          epsilon_start=EPS_START,\n",
    "                                          epsilon_decay=EPS_DECAY,\n",
    "                                          device=device)\n",
    "\n",
    "    # Do step\n",
    "    observation, reward, done, info = env.step(action.item())\n",
    "\n",
    "    # Shape reward\n",
    "    original_reward = reward\n",
    "    shaped_reward = reward\n",
    "    \n",
    "    # Retrieve current screen\n",
    "    screen = observation\n",
    "\n",
    "    # Iterate over all reward shaping mechanisms\n",
    "    for reward_shaping in REWARD_SHAPINGS:\n",
    "        if reward_shaping[\"arguments\"][\"additional_reward\"] != 0:\n",
    "            shaped_reward += reward_shaping[\"method\"](environment=ENVIRONMENT,\n",
    "                                                      screen=screen,\n",
    "                                                      reward=reward,\n",
    "                                                      done=done,\n",
    "                                                      info=info,\n",
    "                                                      **reward_shaping[\"arguments\"])\n",
    "\n",
    "    # # Plot intermediate screen\n",
    "    # if total_frames % 50 == 0:\n",
    "    #     InputExtractor.plot_screen(InputExtractor.get_sharp_screen(env=env, device=device), \"Frame \" + str(\n",
    "    #         total_frames) + \" / shaped reward \" + str(round(shaped_reward, 4)))\n",
    "        \n",
    "    # Use shaped reward for further processing\n",
    "    reward = shaped_reward\n",
    "\n",
    "    # Add reward to episode reward\n",
    "    episode_original_reward += original_reward\n",
    "    episode_shaped_reward += shaped_reward\n",
    "\n",
    "    # Transform reward into a tensor\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "\n",
    "    # Observe new state\n",
    "    last_screen = current_screen\n",
    "    current_screen = InputExtractor.get_screen(env=env, device=device)\n",
    "\n",
    "    # Update next state\n",
    "    next_state = current_screen - last_screen\n",
    "\n",
    "    # Store the transition in memory\n",
    "    memory.push(state, action, next_state, reward)\n",
    "\n",
    "    # Move to the next state\n",
    "    state = next_state\n",
    "\n",
    "    # Perform one step of the optimization (on the target network)\n",
    "    loss = ModelOptimizer.optimize_model(policy_net=policy_net,\n",
    "                                         target_net=target_net,\n",
    "                                         optimizer=optimizer,\n",
    "                                         memory=memory,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         gamma=GAMMA,\n",
    "                                         device=device)\n",
    "    \n",
    "    # Write values to TensorBoard\n",
    "    tensorboard_summary_writer.add_scalar(\"Loss\", loss, total_frames)\n",
    "\n",
    "    if done:    \n",
    "        # Track episode time\n",
    "        episode_end_time = time.time()\n",
    "        episode_duration = episode_end_time - episode_start_time\n",
    "        total_duration = episode_end_time - total_start_time\n",
    "\n",
    "        # Add rewards to total reward\n",
    "        total_original_rewards.append(episode_original_reward)\n",
    "        total_shaped_rewards.append(episode_shaped_reward)\n",
    "        \n",
    "        # Write values to TensorBoard\n",
    "        tensorboard_summary_writer.add_scalar(\"Reward (original)\", episode_original_reward, total_episodes)\n",
    "        tensorboard_summary_writer.add_scalar(\"Reward (shaped)\", episode_shaped_reward, total_episodes)\n",
    "        tensorboard_summary_writer.add_scalar(\"Episode duration\", episode_duration, total_episodes)\n",
    "\n",
    "        if loss is not None:\n",
    "            PerformanceLogger.log_episode(directory=OUTPUT_DIRECTORY + RUN_DIRECTORY,\n",
    "                                            total_episodes=total_episodes + 1,\n",
    "                                            total_frames=total_frames,\n",
    "                                            total_duration=total_duration,\n",
    "                                            total_original_rewards=total_original_rewards,\n",
    "                                            total_shaped_rewards=total_shaped_rewards,\n",
    "                                            episode_frames=episode_frames + 1,\n",
    "                                            episode_original_reward=episode_original_reward,\n",
    "                                            episode_shaped_reward=episode_shaped_reward,\n",
    "                                            episode_loss=loss.item(),\n",
    "                                            episode_duration=episode_duration)\n",
    "\n",
    "      # Update the target network, copying all weights and biases from policy net into target net\n",
    "        if total_episodes % TARGET_UPDATE_RATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if total_episodes % MODEL_SAVE_RATE == 0:\n",
    "            # Save output\n",
    "            ModelStorage.saveModel(directory=OUTPUT_DIRECTORY + RUN_DIRECTORY,\n",
    "                                   total_frames=total_frames,\n",
    "                                   total_episodes=total_episodes,\n",
    "                                   net=target_net,\n",
    "                                   optimizer=optimizer,\n",
    "                                   memory=memory,\n",
    "                                   loss=loss,\n",
    "                                   environment=ENVIRONMENT,\n",
    "                                   environment_wrappers=ENVIRONMENT_WRAPPERS,\n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   gamma=GAMMA,\n",
    "                                   eps_start=EPS_START,\n",
    "                                   eps_end=EPS_END,\n",
    "                                   eps_decay=EPS_DECAY,\n",
    "                                   num_atoms=NUM_ATOMS,\n",
    "                                   vmin=VMIN,\n",
    "                                   vmax=VMAX,\n",
    "                                   target_update_rate=TARGET_UPDATE_RATE,\n",
    "                                   model_save_rate=MODEL_SAVE_RATE,\n",
    "                                   replay_memory_size=REPLAY_MEMORY_SIZE,\n",
    "                                   num_frames=NUM_FRAMES,\n",
    "                                   reward_pong_player_racket_hits_ball=REWARD_PONG_PLAYER_RACKET_HITS_BALL,\n",
    "                                   reward_pong_player_racket_covers_ball=REWARD_PONG_PLAYER_RACKET_COVERS_BALL,\n",
    "                                   reward_pong_player_racket_close_to_ball_linear=REWARD_PONG_PLAYER_RACKET_CLOSE_TO_BALL_LINEAR,\n",
    "                                   reward_pong_player_racket_close_to_ball_quadratic=REWARD_PONG_PLAYER_RACKET_CLOSE_TO_BALL_QUADRATIC,\n",
    "                                   reward_pong_opponent_racket_hits_ball=REWARD_PONG_OPPONENT_RACKET_HITS_BALL,\n",
    "                                   reward_pong_opponent_racket_covers_ball=REWARD_PONG_OPPONENT_RACKET_COVERS_BALL,\n",
    "                                   reward_pong_opponent_racket_close_to_ball_linear=REWARD_PONG_OPPONENT_RACKET_CLOSE_TO_BALL_LINEAR,\n",
    "                                   reward_pong_opponent_racket_close_to_ball_quadratic=REWARD_PONG_OPPONENT_RACKET_CLOSE_TO_BALL_QUADRATIC,\n",
    "                                   reward_breakout_player_racket_hits_ball=REWARD_BREAKOUT_PLAYER_RACKET_HITS_BALL,\n",
    "                                   reward_breakout_player_racket_covers_ball=REWARD_BREAKOUT_PLAYER_RACKET_COVERS_BALL,\n",
    "                                   reward_breakout_player_racket_close_to_ball_linear=REWARD_BREAKOUT_PLAYER_RACKET_CLOSE_TO_BALL_LINEAR,\n",
    "                                   reward_breakout_player_racket_close_to_ball_quadratic=REWARD_BREAKOUT_PLAYER_RACKET_CLOSE_TO_BALL_QUADRATIC,\n",
    "                                   reward_spaceinvaders_player_avoids_line_of_fire=REWARD_SPACEINVADERS_PLAYER_AVOIDS_LINE_OF_FIRE,\n",
    "                                   reward_freeway_chicken_vertical_position=REWARD_FREEWAY_CHICKEN_VERTICAL_POSITION\n",
    "                                   )\n",
    "        \n",
    "            PerformancePlotter.save_values_plot(directory=OUTPUT_DIRECTORY + RUN_DIRECTORY,\n",
    "                                        total_frames=total_frames,\n",
    "                                        values=total_original_rewards,\n",
    "                                        title=\"original rewards\",\n",
    "                                        xlabel=\"reward\",\n",
    "                                        ylabel=\"episode\")\n",
    "\n",
    "            PerformancePlotter.save_values_plot(directory=OUTPUT_DIRECTORY + RUN_DIRECTORY,\n",
    "                                                total_frames=total_frames,\n",
    "                                                values=total_shaped_rewards,\n",
    "                                                title=\"shaped rewards\",\n",
    "                                                xlabel=\"reward\",\n",
    "                                                ylabel=\"episode\")\n",
    "\n",
    "            PerformancePlotter.save_values_plot(directory=OUTPUT_DIRECTORY + RUN_DIRECTORY,\n",
    "                                                total_frames=total_frames,\n",
    "                                                values=total_losses,\n",
    "                                                title=\"losses\",\n",
    "                                                xlabel=\"loss\",\n",
    "                                                ylabel=\"frame\")\n",
    "\n",
    "            ScreenPlotter.save_screen_plot(directory=OUTPUT_DIRECTORY + RUN_DIRECTORY,\n",
    "                                           total_frames=total_frames,\n",
    "                                           env=env,\n",
    "                                           title=\"screenshot\",\n",
    "                                           device=device)\n",
    "\n",
    "        # Reset episode variables\n",
    "        episode_frames = 0\n",
    "        episode_original_reward = 0\n",
    "        episode_shaped_reward = 0\n",
    "        episode_start_time = time.time()\n",
    "        \n",
    "        # Reset the environment and state\n",
    "        env.reset()\n",
    "        last_screen = InputExtractor.get_screen(env=env, device=device)\n",
    "        current_screen = InputExtractor.get_screen(env=env, device=device)\n",
    "        state = current_screen - last_screen\n",
    "\n",
    "        # Increment counter\n",
    "        total_episodes += 1\n",
    "\n",
    "    # Increment counter\n",
    "    episode_frames += 1\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}